{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oQpyYG8sfRV"
   },
   "source": [
    "# Plant Seedlings Classification / First experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJolK2DAoHFK"
   },
   "source": [
    "#Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQxYdsX2knOX"
   },
   "source": [
    "Mounting drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2101,
     "status": "ok",
     "timestamp": 1572141731921,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "2GFXiVkUiGok",
    "outputId": "bf0f92a1-57b5-4406-a2fd-7b52a8cf980c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a78rF-9-kths"
   },
   "source": [
    "Some linux commands to arrange files and directories if it is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3000,
     "status": "ok",
     "timestamp": 1572141771812,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "toM48gdlmYY_",
    "outputId": "4e24387b-768b-4db3-cc8b-1e97163463d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0021e90e4.png  33748968f.png  65e97117e.png  98d819587.png  cb496f36e.png\n",
      "003d61042.png  338c7e907.png  664194d19.png  98da6ef4e.png  cb76a7766.png\n",
      "007b3da8b.png  34dd57ca9.png  6680836dd.png  99036c51d.png  cbba27d89.png\n",
      "0086a6340.png  3526b05cc.png  668c1007c.png  99569b224.png  cbe761896.png\n",
      "00c47e980.png  35a90f8d0.png  66ab0e8d0.png  995c7ab1e.png  cc3d2a59a.png\n",
      "00d090cde.png  35cf9fa01.png  675ec1b0b.png  9a3f20121.png  cc74feadc.png\n",
      "00ef713a8.png  35ebe165c.png  67ce3eaa6.png  9aa5587fe.png  cd5f0db1c.png\n",
      "01291174f.png  36839d5e9.png  67e185673.png  9b4800b42.png  cd6adba97.png\n",
      "026716f9b.png  36d62bf36.png  686dc7ec8.png  9b9911f20.png  ce15eee52.png\n",
      "02cfeb38d.png  36ed4f215.png  6908fb540.png  9baf94467.png  ce3d280eb.png\n",
      "03566743d.png  37297a64c.png  6982a9d30.png  9c0c5b731.png  ce42adffb.png\n",
      "03a2ee656.png  37714071b.png  699d3c707.png  9c32a797e.png  cec5bf198.png\n",
      "03e322a29.png  377283a21.png  69d1669f8.png  9c777333d.png  cf3a8b2fd.png\n",
      "03ef36742.png  37c3108d6.png  6a41bf95b.png  9c8b08a24.png  cf46d09c5.png\n",
      "043449b0b.png  37e545a60.png  6a47821f9.png  9cce7328c.png  cf90fc52d.png\n",
      "0437393b1.png  3827436f3.png  6b721f68e.png  9d3cb4745.png  cfb18d262.png\n",
      "04814f36d.png  38c054379.png  6b9d6f8c9.png  9d79a1f0c.png  cfd8165e9.png\n",
      "05341a8a6.png  391dcd7fd.png  6ba4ef411.png  9df3275da.png  d0152bd7c.png\n",
      "060450d79.png  39858776a.png  6bce55e05.png  9fab816f2.png  d01873fdd.png\n",
      "060f1dc84.png  39b740f7e.png  6be169e41.png  a006a475c.png  d09275360.png\n",
      "0625f063b.png  39c8fde99.png  6c874918c.png  a060c1cf8.png  d09d24c58.png\n",
      "063363305.png  39d569be4.png  6d6eb3830.png  a0b393945.png  d0cdc768f.png\n",
      "06d12f6fa.png  3a909ead8.png  6da892be6.png  a0f37c726.png  d102e1a15.png\n",
      "071cb3ece.png  3abb502fb.png  6db684fff.png  a169b71e7.png  d14aa43f3.png\n",
      "0751c0bbc.png  3b73c3b61.png  6dd095129.png  a19c3faca.png  d17f48d3b.png\n",
      "07e62f903.png  3bbef3ecb.png  6df8e31ea.png  a1da8be3c.png  d2f0f326e.png\n",
      "085974290.png  3d38a87bc.png  6edb96d45.png  a1e0a6c02.png  d2f422ccb.png\n",
      "0885e7690.png  3d65168c2.png  6edc76e7c.png  a254d71f6.png  d2fd9df40.png\n",
      "089ad62a7.png  3d67c434b.png  711b46fba.png  a276c65f7.png  d3331e071.png\n",
      "08d591441.png  3da774107.png  71334c634.png  a2b703e21.png  d350a25fa.png\n",
      "0911d3dee.png  3dd52bd2a.png  71b232519.png  a2c89c367.png  d41d87796.png\n",
      "099b961ec.png  3e9f41817.png  71e73a8a0.png  a2d25b4f3.png  d488a4fe1.png\n",
      "0a64e3e6c.png  3ebbe9ca4.png  71f5323c5.png  a35fd6fbb.png  d515398fd.png\n",
      "0ad9e7dfb.png  3eda9cbb6.png  721b2c47a.png  a38b8a581.png  d563be369.png\n",
      "0ae6668fa.png  3edf5e9ef.png  721be0a4a.png  a3b375b34.png  d5f7dd60a.png\n",
      "0bf7bfb05.png  3eebd36c6.png  728eabae1.png  a3d0031fd.png  d668409ff.png\n",
      "0c27cf05f.png  3efa1f66c.png  73260a4ee.png  a4b61a4ea.png  d689256be.png\n",
      "0c4199daa.png  3f64c2c1b.png  74068643d.png  a544fc46d.png  d6c8c3c48.png\n",
      "0c45ace27.png  3f826b318.png  74d810f87.png  a55d26a4c.png  d6d31dcbe.png\n",
      "0c51bf229.png  3f92d8039.png  74fd477eb.png  a5db42f7d.png  d6d80a321.png\n",
      "0c5f6c493.png  3fbd0fc6a.png  7506c0c02.png  a74bf916d.png  d7017f701.png\n",
      "0caeda5df.png  3fbf1a417.png  752101fdf.png  a74d475c2.png  d84d37a61.png\n",
      "0d117d910.png  4049a6ea6.png  754b1adf8.png  a7bd7cadb.png  d89db156f.png\n",
      "0d31e6602.png  406162ef9.png  756dd5070.png  a800caead.png  d8f4923f8.png\n",
      "0dba99002.png  406ecb5c5.png  75cb95e91.png  a83820a2c.png  d93c7ab6d.png\n",
      "0e8492cb1.png  410e6f702.png  7615e52d3.png  a8388a37f.png  d9c50616e.png\n",
      "0ebf8f2f4.png  41e07778c.png  76555b064.png  a85b48a95.png  da231c97f.png\n",
      "0ee4ad224.png  41f1c3cdb.png  7691014a1.png  a85fc8c9a.png  da4ed3a28.png\n",
      "0f6cbe5e8.png  4287d810c.png  7696badea.png  a890ac088.png  da5255450.png\n",
      "0fb233ad6.png  429211ee6.png  76dbd1054.png  a8b431a3e.png  da713c465.png\n",
      "115f93ecc.png  42e7ed442.png  770a265f5.png  a8c8a1db0.png  da9ef7858.png\n",
      "116b136de.png  4392d93cf.png  77ccb8b2a.png  a8da9c08d.png  dabe3e5be.png\n",
      "1191ba346.png  43ede9de9.png  780bd2a2c.png  a8de7c1b7.png  dabea05f4.png\n",
      "11d3f68ff.png  444473900.png  780defa2e.png  a935ca110.png  dc4cd56a3.png\n",
      "122913909.png  446f7da01.png  785a73ab8.png  a93f940d6.png  dc55449b2.png\n",
      "12625488b.png  44e8b8833.png  78750e0ff.png  a9d2eab61.png  dcd7ff249.png\n",
      "126a71ce0.png  456d507c0.png  78b1bf91a.png  aa7d098d1.png  dce2f6612.png\n",
      "1312065a5.png  466bb6d3b.png  78c5fba1d.png  aad8375e0.png  dd5ec63d9.png\n",
      "1364b297a.png  46c14fde2.png  79d93bc96.png  aaf4da98f.png  dd9f36df7.png\n",
      "1376f3b63.png  47b7d8e17.png  79dafec17.png  ab0f67743.png  de0b79659.png\n",
      "13b9fa92d.png  47f9e5d91.png  79e5ea8fa.png  ab35453cb.png  df11d56a7.png\n",
      "1459e96a0.png  48231e475.png  79fba50db.png  abc331628.png  df521c0c0.png\n",
      "148bbda66.png  4823c3ffa.png  7a38416be.png  abf8b0772.png  df7cb5f87.png\n",
      "14bb43eee.png  486e59179.png  7b21ba6ba.png  ac3193f78.png  dfb1d9012.png\n",
      "1541bdb2e.png  48d97c645.png  7b52585da.png  ac75d3326.png  e0ec5b6a1.png\n",
      "1623fb9e1.png  48ef6a2ff.png  7beb2766f.png  acdb75e00.png  e14afa235.png\n",
      "16357b436.png  490c4f9c8.png  7c85b0265.png  ace8761dd.png  e15472085.png\n",
      "16467a950.png  4a337a4a9.png  7cabd68cc.png  ad12382d4.png  e15fce4f2.png\n",
      "165681fd9.png  4ac29bbf0.png  7d22abf91.png  adb7a032c.png  e1809cef2.png\n",
      "1694a70e4.png  4b032563b.png  7d3045fc3.png  ae90f2827.png  e19673dc9.png\n",
      "16fd2e01a.png  4b155fb07.png  7d4cd07ad.png  aecfaed64.png  e19ad6ac9.png\n",
      "172f9b10b.png  4bbf1f6ea.png  7e9cf1c46.png  aee6fa3df.png  e1a0e3202.png\n",
      "17529c555.png  4bbfd1e05.png  7f31c7f42.png  af45e222a.png  e1abb4ff9.png\n",
      "1758a1baf.png  4bcdaa5e2.png  7f46a71db.png  afa446484.png  e3f50adfc.png\n",
      "177d7e2a4.png  4c5ab9b68.png  7f9e9565d.png  afcf6abd5.png  e471f1d3a.png\n",
      "17a78fb44.png  4c7838de4.png  7fdb7202d.png  b026bf8ca.png  e478c452c.png\n",
      "17d5e5ac4.png  4c8005bbc.png  800a8c17e.png  b03397525.png  e4a76885b.png\n",
      "1821eb11a.png  4e1190d78.png  808578ed5.png  b0acaff4a.png  e4d5ec761.png\n",
      "187668bde.png  4e18ab737.png  808cf55c6.png  b130a0632.png  e5064f6be.png\n",
      "1926e82fd.png  4e69d100a.png  809eb0b82.png  b145ba9d4.png  e52493d0b.png\n",
      "19618ad6a.png  4e9d3765f.png  80e299ae9.png  b1cd2a91e.png  e5297b675.png\n",
      "19b51843a.png  4ea7493d5.png  8104422bb.png  b215531dd.png  e5368474f.png\n",
      "19e58cc5e.png  4f44ca525.png  8170d33c1.png  b2706e2b3.png  e5881dd33.png\n",
      "19fdf19fb.png  4f83143e1.png  824f5d4e5.png  b29339405.png  e5e3dccff.png\n",
      "1b490196c.png  502dff972.png  827279bad.png  b30ab4659.png  e6f1211a2.png\n",
      "1b6a6494d.png  506347cfe.png  82b5f4d33.png  b31292706.png  e7077322d.png\n",
      "1be0713da.png  50de8a115.png  8301b0547.png  b341d0aab.png  e721c6ac8.png\n",
      "1bf9b94a6.png  521b27a17.png  8303b27ed.png  b39c71707.png  e73e308be.png\n",
      "1c52ea820.png  523e5505c.png  8311740de.png  b3d6fdb80.png  e783f5a4f.png\n",
      "1c680883c.png  5296835a0.png  835dc5447.png  b3e08b037.png  e80a259c5.png\n",
      "1cfd91582.png  52a87abe5.png  837ac0270.png  b47691c08.png  e82017baa.png\n",
      "1d0cbd819.png  52dc7a4d6.png  8455169fe.png  b4c3df835.png  e84464f5a.png\n",
      "1d321253f.png  5315c2dca.png  851c90831.png  b4f7c9214.png  e88bf0db9.png\n",
      "1d48b7564.png  534e74d83.png  85431c075.png  b573b7a56.png  e901b0f28.png\n",
      "1d56351b2.png  539961189.png  855955aaf.png  b5c7fd009.png  e921021a8.png\n",
      "1dc7c45df.png  53ceb4657.png  856f2910a.png  b62dca166.png  e96e57a90.png\n",
      "1e095a7e1.png  53e6e9000.png  8585f9718.png  b687160f5.png  e98e5d1d5.png\n",
      "1f290e016.png  54b3afd58.png  862b8e7a0.png  b6a3f7876.png  e9cd91682.png\n",
      "1f3f44563.png  54c8bb900.png  86676d627.png  b6f3d8b5d.png  e9d48d664.png\n",
      "1f5e5554e.png  550a8b7e6.png  866be78b0.png  b7192c70f.png  eaf0815e2.png\n",
      "1fefb54b7.png  55251925f.png  86c309150.png  b7a7f6390.png  ec08a5d56.png\n",
      "205df1df3.png  558aa7deb.png  86f08e6d1.png  b7ad92859.png  ede6b84b4.png\n",
      "20817c846.png  55920f07f.png  87608f7aa.png  b828443ff.png  edfdb4aeb.png\n",
      "20e562fd5.png  55a852f40.png  87f627bf9.png  b9062c1c8.png  eec1079a1.png\n",
      "20ea96bcc.png  55fed435f.png  8874bba69.png  b944a49ca.png  eef131644.png\n",
      "20f983a71.png  56112b92c.png  88ac6df54.png  b98327bf4.png  ef02b4ee7.png\n",
      "2126dc71b.png  56a01b835.png  88d8a4508.png  ba3ce6b3e.png  ef3e232ad.png\n",
      "219fd68d5.png  5779fe8b4.png  8916793ce.png  bb1c84bbc.png  ef65533d5.png\n",
      "223e4af09.png  5817b766d.png  892e9d6c6.png  bb1d1bfd3.png  ef74dbcad.png\n",
      "22e79540f.png  5883b423d.png  897e5a8de.png  bb20fce02.png  ef7a5651d.png\n",
      "22fbf13d6.png  589e643b8.png  8a30b2de3.png  bb64660b7.png  ef9676433.png\n",
      "239bdf640.png  590f5aea6.png  8a32d0bfa.png  bb7621cb3.png  efe19dc32.png\n",
      "23bc8ec4f.png  592473c83.png  8a4d3a1b1.png  bd72d4d8a.png  f0ffa00bd.png\n",
      "23e480e64.png  592cc5b89.png  8a8d6c712.png  bd789d151.png  f1e87cba7.png\n",
      "2406d6c99.png  59358cd44.png  8ab8a958b.png  bdde957ec.png  f1f7c833f.png\n",
      "241e6935a.png  593896f83.png  8b043093d.png  be2499cf4.png  f23faf9c1.png\n",
      "248436078.png  599691cd9.png  8b27bfd2b.png  be341dbdc.png  f25996db8.png\n",
      "24a058589.png  599c82eea.png  8b9144917.png  bea23d9f8.png  f2dc546ca.png\n",
      "24c94a6ca.png  59b2c6f2b.png  8bc0261c9.png  bebcaab66.png  f33e9d918.png\n",
      "24d36c52c.png  59c6a9f95.png  8c98a6e9b.png  beebe5f4e.png  f351ce097.png\n",
      "24d78df74.png  59e1cea8d.png  8c9953903.png  bf3924a57.png  f3fcfff1b.png\n",
      "24dbc3b21.png  59f62ad1d.png  8ca6140ca.png  bf66b9cd2.png  f4021df6c.png\n",
      "258b1183c.png  5a38ac566.png  8cf2e3e6c.png  bfab3e3d0.png  f4234cf4f.png\n",
      "25a4c427e.png  5a6bf96f6.png  8cf909eb3.png  bffc08672.png  f445fe6fb.png\n",
      "25cf6eb73.png  5af1d74ee.png  8cfd98117.png  c0461776c.png  f48916a8c.png\n",
      "25fa8d109.png  5b3000b9a.png  8d6991365.png  c069fc3fa.png  f4ad9d950.png\n",
      "26852751a.png  5b3beec58.png  8d6acbe9b.png  c06e7c748.png  f4caf74f9.png\n",
      "2693e5c65.png  5b63dcc21.png  8db450ce3.png  c0bc3997b.png  f4e7733d4.png\n",
      "26e7ae885.png  5bbc0a255.png  8dbb8e1b9.png  c0d9e170b.png  f593c9cf0.png\n",
      "270b939cf.png  5bc6595f6.png  8e29abce1.png  c0f5d9ac8.png  f66ae4070.png\n",
      "279df95f2.png  5bd71f445.png  8e2e5604e.png  c10ccbd82.png  f6d250856.png\n",
      "288564c76.png  5bdcfa329.png  8e3ed0a25.png  c10db7ae2.png  f8318faf1.png\n",
      "29bab7cad.png  5c3cd7ea2.png  8e4eaeec0.png  c1ecff98b.png  f85ed9b6d.png\n",
      "29ce426a1.png  5ca0205f9.png  8e6ec1ca6.png  c26ccf73c.png  f9b6bfb00.png\n",
      "29f49cd0b.png  5ca2687a4.png  8ece6efec.png  c2de6020a.png  f9ea23fb5.png\n",
      "2a5064f19.png  5dcad9a53.png  8f523520c.png  c35efa095.png  f9f35cbd4.png\n",
      "2a667e099.png  5e6a237f2.png  8faadb6a8.png  c4ed8ed38.png  fa5fd1384.png\n",
      "2b55a2da2.png  5eb9c26a6.png  90b595f12.png  c50335991.png  fa9f3a8f9.png\n",
      "2bd74d2da.png  5ee9d0a5b.png  90d119d25.png  c5078bac5.png  fadc6adbc.png\n",
      "2d5058a59.png  5f04aed97.png  91e469b4a.png  c5e419015.png  fb022edf9.png\n",
      "2d992d1fb.png  6049234e6.png  92292055d.png  c5e88cd42.png  fba8fc78a.png\n",
      "2d9c798f9.png  604dd663f.png  93079d970.png  c60e91e07.png  fbf88b6be.png\n",
      "2dd5cfba9.png  606647f64.png  9326bda1b.png  c63da993b.png  fc3e58836.png\n",
      "2df78338c.png  60ee66ddd.png  93d76fd5d.png  c64370a72.png  fc6f686fb.png\n",
      "2e86f1085.png  60f0bc617.png  948cdb277.png  c6b76307d.png  fd253a74e.png\n",
      "2ea664465.png  60fea2ef6.png  9516e56c4.png  c6c8d4ba0.png  fd87b36ae.png\n",
      "2f0004a7f.png  615d2b0a9.png  953496deb.png  c7051c902.png  fd925f542.png\n",
      "2f246d688.png  618de3d7a.png  958bb9e96.png  c74c5b7fc.png  fda0b5c38.png\n",
      "2ff5cb348.png  61b044411.png  963544aa0.png  c75a82234.png  fda39e16f.png\n",
      "30ad31220.png  61dd2cdc5.png  9643fc5f4.png  c7ae30f3a.png  fdea6b119.png\n",
      "3185294c8.png  632156793.png  966ae5ad9.png  c7b07431e.png  fe29629fb.png\n",
      "31f3dd81f.png  63c07d340.png  969a851be.png  c7eb96871.png  fe9e87b78.png\n",
      "31fcdc161.png  647689543.png  96ecad7a1.png  c832e4302.png  fea1d13d6.png\n",
      "3281183f9.png  64fe8beb9.png  96f14d90c.png  c85ef220d.png  fea355851.png\n",
      "32a8c8a1d.png  653193c1a.png  974959ec1.png  c88ebfb47.png  fea3da57c.png\n",
      "32b42c120.png  65489944f.png  976e4e079.png  c8f50f0c3.png  fef2ade8c.png\n",
      "32c86784b.png  659412b1a.png  97844bfd5.png  caa2fbd79.png  ff65bc002.png\n",
      "33317fc2a.png  65d08b894.png  97b2f0a10.png  cadab6616.png  ffc6f8527.png\n",
      "33448fe39.png  65e262a6d.png  98062cd87.png  cae684f8f.png\n"
     ]
    }
   ],
   "source": [
    "! ls \"/content/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5BMb-A5k39G"
   },
   "source": [
    "Check if GPU is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2507,
     "status": "ok",
     "timestamp": 1572136234097,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "ml18aVm_TvG3",
    "outputId": "7c729144-4120-404b-c506-66c16c985e93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jwvi24p_k9pl"
   },
   "source": [
    "Import libraries ( more details about in the report and the notebook of second experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWT79OC9iGoo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "torch.manual_seed(23)\n",
    "random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0_QS_0FU21v"
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsLLLYC3lQaC"
   },
   "source": [
    "Set some constant to access a file and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7PGWGmWU3rs"
   },
   "outputs": [],
   "source": [
    "# path to original dataset\n",
    "original_dataset_dir = '/content/gdrive/My Drive/train'\n",
    "# directory to store the smaller dataset\n",
    "base_dir = '/content/gdrive/My Drive/' \n",
    "\n",
    "#os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvTGDGJNU7pV"
   },
   "outputs": [],
   "source": [
    "# make training, validation and test set directories\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "#os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "#os.mkdir(validation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-RhozirlX_D"
   },
   "source": [
    "#Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90XxDSEWVBQJ"
   },
   "outputs": [],
   "source": [
    "classes = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "           'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvAdQYK4VCTy"
   },
   "outputs": [],
   "source": [
    "for directory in [train_dir, validation_dir]:\n",
    "    for clas in classes:\n",
    "        clas_dir = os.path.join(directory, clas)\n",
    "        #os.mkdir(clas_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1572136250584,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "5PEICdepVFhO",
    "outputId": "f7715e5a-66bb-4c27-c0ca-1022d8cd1cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black-grass :  263\n",
      "Charlock :  390\n",
      "Cleavers :  287\n",
      "Common Chickweed :  611\n",
      "Common wheat :  221\n",
      "Fat Hen :  475\n",
      "Loose Silky-bent :  654\n",
      "Maize :  221\n",
      "Scentless Mayweed :  516\n",
      "Shepherds Purse :  231\n",
      "Small-flowered Cranesbill :  496\n",
      "Sugar beet :  385\n"
     ]
    }
   ],
   "source": [
    "for clas in classes:\n",
    "    print(clas, \": \", len(os.listdir(os.path.join(original_dataset_dir, clas))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdAretfwltcE"
   },
   "source": [
    "#Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0brnoJ8lzGK"
   },
   "source": [
    "Applyng several data augmentation techniques, image resizing, center crop and random horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcw1T8nGiGor"
   },
   "outputs": [],
   "source": [
    "def get_transforms(target_size=100, normalize=False):\n",
    "    t = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4faGv2OVTYk"
   },
   "outputs": [],
   "source": [
    "path = '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2bK3mvDmLaX"
   },
   "source": [
    "Apply data augmentation in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1572136280506,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "MO1XbCJHiGot",
    "outputId": "c132f67b-dedf-4801-bb3c-66c7ac52fb6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has:\n",
      "  4750 elements\n",
      "  12 classes\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 4750\n",
      "    Root location: /content/gdrive/My Drive/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=100, interpolation=PIL.Image.BILINEAR)\n",
      "               CenterCrop(size=(100, 100))\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "full_dataset = torchvision.datasets.ImageFolder('/content/gdrive/My Drive/train', transform=get_transforms())\n",
    "print('This dataset has:')\n",
    "print('  {} elements'.format(len(full_dataset)))\n",
    "print('  {} classes'.format(len(full_dataset.classes)))\n",
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOdWt5tWmSad"
   },
   "source": [
    "Splitting the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-QWc-cGmVGY"
   },
   "source": [
    "Training set = 70%\n",
    "Validation set=30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1572136286096,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "oZ4oOyQsiGoz",
    "outputId": "d8bbec33-be45-4cf2-a7d2-17ae939c075d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 3325 elements\n",
      "Validation dataset contains 1425 elements\n"
     ]
    }
   ],
   "source": [
    "train_len = int(0.7 * len(full_dataset))\n",
    "validate_len = len(full_dataset) - train_len\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(full_dataset, (train_len, validate_len))\n",
    "print('Training dataset contains {} elements'.format(len(train_dataset)))\n",
    "print('Validation dataset contains {} elements'.format(len(validate_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTDfI0-jmd6H"
   },
   "source": [
    "Set train and validation set as dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05pD-64XiGo4"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKNChKlrm7qV"
   },
   "source": [
    "#Building model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQrULS3Dm3Jy"
   },
   "source": [
    "Using a version of LeNet-5, modified to take 3-channel color images (instead of 1-channel images as it was originally defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRk6LHVVpT7K"
   },
   "source": [
    "The __init__() method defines two convolutional layers and three linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nQM6F_Dpc6H"
   },
   "source": [
    "The forward() method composes these layers and some important functions into a computation graph that takes in a 3x32x32 tensor representing a 3-color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HICRmu9cUAR4"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsFzQsndpnCF"
   },
   "source": [
    "skeleton of the image classifier (model architecture).Start with 3 color inputs and finish with 12 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJ0AaaCplKIb"
   },
   "outputs": [],
   "source": [
    "class SeedlingModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeedlingModelV1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 22 * 22, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uR0lLtgRqESW"
   },
   "source": [
    "Instantiates the model.Extracts an instance from the dataset\n",
    "feeds the instance to the model for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1572136303435,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "A6hFeBiDmDJe",
    "outputId": "124122ae-fab0-4cda-879a-729ae2398c10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0380, -0.0902,  0.0075,  0.1225, -0.0666,  0.0214,  0.0193, -0.1222,\n",
      "         -0.0223,  0.0353, -0.0416, -0.0931]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = SeedlingModelV1()\n",
    "image, label = train_dataset[0]\n",
    "output = model(torch.unsqueeze(image, 0))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNd5MFM7qSTi"
   },
   "source": [
    "Training the Model\n",
    "\n",
    "Setting some constants for tunning them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3xBXg0oiGpE"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 20 # number of passes over the training dataset\n",
    "LR = 0.01 # learning rate\n",
    "MOMENTUM = 0.5 # for SGD\n",
    "\n",
    "BATCH_SIZE = 4 # number of instances per batch served by dataloader\n",
    "NUM_WORKERS = 2 # number of I/O threads used by dataloader\n",
    "\n",
    "MODEL_DIR = 'models' # save models here\n",
    "MODEL_SAVEFILE = 'seedling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMWAvfZsqm8k"
   },
   "source": [
    "Create folder for store models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3648,
     "status": "ok",
     "timestamp": 1572136314387,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "9kzvIlXZiGpH",
    "outputId": "6ad78030-df84-4aed-a817-b0a99face5b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5pgCYxRqwjm"
   },
   "source": [
    "Registering information about the training and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ajj-fKBniGpJ"
   },
   "outputs": [],
   "source": [
    "def tlog(msg):\n",
    "    print('{}   {}'.format(time.asctime(), msg))\n",
    "\n",
    "    \n",
    "def save_model(model, epoch):\n",
    "    tlog('Saving model')\n",
    "    savefile = \"{}-e{}-{}.pt\".format(MODEL_SAVEFILE, epoch, int(time.time()))\n",
    "    path = os.path.join(MODEL_DIR, savefile)\n",
    "    # recommended way from https://pytorch.org/docs/stable/notes/serialization.html\n",
    "    torch.save(model.state_dict(), path)\n",
    "    return savefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEhjGLhEq94c"
   },
   "source": [
    "Checking if the GPU is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1572136322483,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "xASxL1_7iGpM",
    "outputId": "846a9f43-f8d4-42b4-f855-1dedd8901c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ready to go!\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    print('*** GPU not available - running on CPU. ***')\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_W9UKIA_rM0a"
   },
   "source": [
    "Recreate the key components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-ONDC4AiGpQ"
   },
   "outputs": [],
   "source": [
    "full_dataset = torchvision.datasets.ImageFolder('/content/gdrive/My Drive/train', transform=get_transforms())\n",
    "train_len = int(0.8 * len(full_dataset))\n",
    "validate_len = len(full_dataset) - train_len\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(full_dataset, (train_len, validate_len))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=1)\n",
    "\n",
    "model = SeedlingModelV1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOgtu29vrTRb"
   },
   "source": [
    " Training loop function (details comment inside) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VucrAaNmiGpW"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs=N_EPOCHS):\n",
    "    tlog('Training the model...')\n",
    "    tlog('working on {}'.format(device))\n",
    "    \n",
    "    best_accuracy = 0. # determines whether we save a copy of the model\n",
    "    saved_model_filename = None\n",
    "    \n",
    "    model = model.to(device) # move to GPU if available\n",
    "    loss_fn = nn.CrossEntropyLoss() # combines nn.LogSoftmax() and nn.NLLLoss() for classification tasks\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tlog('BEGIN EPOCH {} of {}'.format(epoch + 1, epochs))\n",
    "        running_loss = 0. # bookkeeping\n",
    "        \n",
    "        tlog('Train:')\n",
    "        for i, data in enumerate(train_loader):\n",
    "            instances, labels = data[0], data[1]\n",
    "            instances, labels = instances.to(device), labels.to(device) # move to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            guesses = model(instances)\n",
    "            loss = loss_fn(guesses, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 200 == 0: # log every 200 batches\n",
    "                tlog('  batch {}   avg loss: {}'.format(i + 1, running_loss / (200)))\n",
    "                running_loss = 0.\n",
    "        \n",
    "        tlog('Validate:')\n",
    "        with torch.no_grad(): # no need to do expensive gradient computation for validation\n",
    "            total_loss = 0.\n",
    "            correct = 0\n",
    "            \n",
    "            for i, data in enumerate(validate_loader):\n",
    "                instance, label = data[0], data[1]\n",
    "                instance, label = instance.to(device), label.to(device) # move to GPU if available\n",
    "                \n",
    "                guess = model(instance)\n",
    "                loss = loss_fn(guess, label)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                prediction = torch.argmax(guess, 1)\n",
    "                if prediction.item() == label.item(): # assuming batch size of 1\n",
    "                    correct += 1\n",
    "\n",
    "            avg_loss = total_loss / len(validate_loader)\n",
    "            accuracy = correct / len(validate_loader)\n",
    "            tlog('  Avg loss for epoch: {}   accuracy: {}'.format(avg_loss, accuracy))\n",
    "            \n",
    "            if accuracy >= best_accuracy:\n",
    "                tlog( '  New accuracy peak, saving model')\n",
    "                best_accuracy = accuracy\n",
    "                saved_model_filename = save_model(model, epoch + 1)\n",
    "                \n",
    "    return (saved_model_filename, best_accuracy)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDzgoWalrckC"
   },
   "source": [
    "Print avg loss and accuracy for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2405981,
     "status": "ok",
     "timestamp": 1572138745342,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "PUKMsjOxiGpb",
    "outputId": "d7392152-3805-40db-a0c9-a1e6ae712153",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 27 00:32:19 2019   Training the model...\n",
      "Sun Oct 27 00:32:19 2019   working on cuda\n",
      "Sun Oct 27 00:32:21 2019   BEGIN EPOCH 1 of 20\n",
      "Sun Oct 27 00:32:21 2019   Train:\n",
      "Sun Oct 27 00:35:00 2019     batch 200   avg loss: 2.4632503211498262\n",
      "Sun Oct 27 00:37:49 2019     batch 400   avg loss: 2.4362348598241805\n",
      "Sun Oct 27 00:40:41 2019     batch 600   avg loss: 2.4246667897701264\n",
      "Sun Oct 27 00:43:30 2019     batch 800   avg loss: 2.4216341572999953\n",
      "Sun Oct 27 00:45:40 2019   Validate:\n",
      "Sun Oct 27 00:52:18 2019     Avg loss for epoch: 2.4198215765702096   accuracy: 0.13473684210526315\n",
      "Sun Oct 27 00:52:18 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:52:18 2019   Saving model\n",
      "Sun Oct 27 00:52:18 2019   BEGIN EPOCH 2 of 20\n",
      "Sun Oct 27 00:52:18 2019   Train:\n",
      "Sun Oct 27 00:52:29 2019     batch 200   avg loss: 2.41332747399807\n",
      "Sun Oct 27 00:52:39 2019     batch 400   avg loss: 2.4425433552265168\n",
      "Sun Oct 27 00:52:49 2019     batch 600   avg loss: 2.411971354484558\n",
      "Sun Oct 27 00:52:58 2019     batch 800   avg loss: 2.416982743740082\n",
      "Sun Oct 27 00:53:06 2019   Validate:\n",
      "Sun Oct 27 00:53:22 2019     Avg loss for epoch: 2.419850431492454   accuracy: 0.13473684210526315\n",
      "Sun Oct 27 00:53:22 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:53:22 2019   Saving model\n",
      "Sun Oct 27 00:53:22 2019   BEGIN EPOCH 3 of 20\n",
      "Sun Oct 27 00:53:22 2019   Train:\n",
      "Sun Oct 27 00:53:32 2019     batch 200   avg loss: 2.427896795272827\n",
      "Sun Oct 27 00:53:42 2019     batch 400   avg loss: 2.418172023296356\n",
      "Sun Oct 27 00:53:52 2019     batch 600   avg loss: 2.432005915641785\n",
      "Sun Oct 27 00:54:02 2019     batch 800   avg loss: 2.3925878828763962\n",
      "Sun Oct 27 00:54:09 2019   Validate:\n",
      "Sun Oct 27 00:54:25 2019     Avg loss for epoch: 2.371110689891012   accuracy: 0.1431578947368421\n",
      "Sun Oct 27 00:54:25 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:54:25 2019   Saving model\n",
      "Sun Oct 27 00:54:25 2019   BEGIN EPOCH 4 of 20\n",
      "Sun Oct 27 00:54:25 2019   Train:\n",
      "Sun Oct 27 00:54:36 2019     batch 200   avg loss: 2.3465223044157026\n",
      "Sun Oct 27 00:54:46 2019     batch 400   avg loss: 2.264667531847954\n",
      "Sun Oct 27 00:54:55 2019     batch 600   avg loss: 2.0376757457852364\n",
      "Sun Oct 27 00:55:05 2019     batch 800   avg loss: 1.7888764634728431\n",
      "Sun Oct 27 00:55:12 2019   Validate:\n",
      "Sun Oct 27 00:55:29 2019     Avg loss for epoch: 1.6691344159527828   accuracy: 0.42947368421052634\n",
      "Sun Oct 27 00:55:29 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:55:29 2019   Saving model\n",
      "Sun Oct 27 00:55:29 2019   BEGIN EPOCH 5 of 20\n",
      "Sun Oct 27 00:55:29 2019   Train:\n",
      "Sun Oct 27 00:55:38 2019     batch 200   avg loss: 1.6616958215832711\n",
      "Sun Oct 27 00:55:48 2019     batch 400   avg loss: 1.6522623711824418\n",
      "Sun Oct 27 00:55:58 2019     batch 600   avg loss: 1.5628476595878602\n",
      "Sun Oct 27 00:56:08 2019     batch 800   avg loss: 1.4603648728132248\n",
      "Sun Oct 27 00:56:16 2019   Validate:\n",
      "Sun Oct 27 00:56:33 2019     Avg loss for epoch: 1.4389127609604284   accuracy: 0.4652631578947368\n",
      "Sun Oct 27 00:56:33 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:56:33 2019   Saving model\n",
      "Sun Oct 27 00:56:33 2019   BEGIN EPOCH 6 of 20\n",
      "Sun Oct 27 00:56:33 2019   Train:\n",
      "Sun Oct 27 00:56:43 2019     batch 200   avg loss: 1.375533367395401\n",
      "Sun Oct 27 00:56:53 2019     batch 400   avg loss: 1.3714644473791122\n",
      "Sun Oct 27 00:57:02 2019     batch 600   avg loss: 1.2944939157366753\n",
      "Sun Oct 27 00:57:13 2019     batch 800   avg loss: 1.2760552605986595\n",
      "Sun Oct 27 00:57:20 2019   Validate:\n",
      "Sun Oct 27 00:57:36 2019     Avg loss for epoch: 1.2725668711411326   accuracy: 0.5684210526315789\n",
      "Sun Oct 27 00:57:36 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:57:36 2019   Saving model\n",
      "Sun Oct 27 00:57:36 2019   BEGIN EPOCH 7 of 20\n",
      "Sun Oct 27 00:57:36 2019   Train:\n",
      "Sun Oct 27 00:57:45 2019     batch 200   avg loss: 1.1482130271196365\n",
      "Sun Oct 27 00:57:56 2019     batch 400   avg loss: 1.2051895451545716\n",
      "Sun Oct 27 00:58:06 2019     batch 600   avg loss: 1.1568367052078248\n",
      "Sun Oct 27 00:58:15 2019     batch 800   avg loss: 1.1381519137322902\n",
      "Sun Oct 27 00:58:23 2019   Validate:\n",
      "Sun Oct 27 00:58:39 2019     Avg loss for epoch: 1.0488278063974883   accuracy: 0.6357894736842106\n",
      "Sun Oct 27 00:58:39 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:58:39 2019   Saving model\n",
      "Sun Oct 27 00:58:39 2019   BEGIN EPOCH 8 of 20\n",
      "Sun Oct 27 00:58:39 2019   Train:\n",
      "Sun Oct 27 00:58:50 2019     batch 200   avg loss: 1.0841704484820367\n",
      "Sun Oct 27 00:58:59 2019     batch 400   avg loss: 1.0887814611196518\n",
      "Sun Oct 27 00:59:10 2019     batch 600   avg loss: 1.031855977475643\n",
      "Sun Oct 27 00:59:19 2019     batch 800   avg loss: 0.9647071027755737\n",
      "Sun Oct 27 00:59:26 2019   Validate:\n",
      "Sun Oct 27 00:59:43 2019     Avg loss for epoch: 1.0055206425566423   accuracy: 0.6726315789473685\n",
      "Sun Oct 27 00:59:43 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 00:59:43 2019   Saving model\n",
      "Sun Oct 27 00:59:43 2019   BEGIN EPOCH 9 of 20\n",
      "Sun Oct 27 00:59:43 2019   Train:\n",
      "Sun Oct 27 00:59:52 2019     batch 200   avg loss: 0.9610844714939595\n",
      "Sun Oct 27 01:00:03 2019     batch 400   avg loss: 0.9296071031689643\n",
      "Sun Oct 27 01:00:14 2019     batch 600   avg loss: 0.9050113236904145\n",
      "Sun Oct 27 01:00:22 2019     batch 800   avg loss: 0.9352481082081795\n",
      "Sun Oct 27 01:00:30 2019   Validate:\n",
      "Sun Oct 27 01:00:46 2019     Avg loss for epoch: 0.9968729419457285   accuracy: 0.6505263157894737\n",
      "Sun Oct 27 01:00:46 2019   BEGIN EPOCH 10 of 20\n",
      "Sun Oct 27 01:00:46 2019   Train:\n",
      "Sun Oct 27 01:00:56 2019     batch 200   avg loss: 0.7698834589123726\n",
      "Sun Oct 27 01:01:06 2019     batch 400   avg loss: 0.8912707851827144\n",
      "Sun Oct 27 01:01:16 2019     batch 600   avg loss: 0.8324303016066551\n",
      "Sun Oct 27 01:01:26 2019     batch 800   avg loss: 0.880192177593708\n",
      "Sun Oct 27 01:01:33 2019   Validate:\n",
      "Sun Oct 27 01:01:49 2019     Avg loss for epoch: 0.9698931267386989   accuracy: 0.671578947368421\n",
      "Sun Oct 27 01:01:49 2019   BEGIN EPOCH 11 of 20\n",
      "Sun Oct 27 01:01:49 2019   Train:\n",
      "Sun Oct 27 01:01:59 2019     batch 200   avg loss: 0.741875765323639\n",
      "Sun Oct 27 01:02:09 2019     batch 400   avg loss: 0.7937079314887524\n",
      "Sun Oct 27 01:02:19 2019     batch 600   avg loss: 0.6544260478019714\n",
      "Sun Oct 27 01:02:29 2019     batch 800   avg loss: 0.7848442122340202\n",
      "Sun Oct 27 01:02:35 2019   Validate:\n",
      "Sun Oct 27 01:02:51 2019     Avg loss for epoch: 0.8836259460449218   accuracy: 0.7021052631578948\n",
      "Sun Oct 27 01:02:51 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 01:02:51 2019   Saving model\n",
      "Sun Oct 27 01:02:51 2019   BEGIN EPOCH 12 of 20\n",
      "Sun Oct 27 01:02:51 2019   Train:\n",
      "Sun Oct 27 01:03:02 2019     batch 200   avg loss: 0.6545965351164341\n",
      "Sun Oct 27 01:03:11 2019     batch 400   avg loss: 0.7279773509502411\n",
      "Sun Oct 27 01:03:22 2019     batch 600   avg loss: 0.7488901564478874\n",
      "Sun Oct 27 01:03:31 2019     batch 800   avg loss: 0.7066481320559979\n",
      "Sun Oct 27 01:03:39 2019   Validate:\n",
      "Sun Oct 27 01:03:55 2019     Avg loss for epoch: 0.9121799339746174   accuracy: 0.6884210526315789\n",
      "Sun Oct 27 01:03:55 2019   BEGIN EPOCH 13 of 20\n",
      "Sun Oct 27 01:03:55 2019   Train:\n",
      "Sun Oct 27 01:04:05 2019     batch 200   avg loss: 0.6340436019003391\n",
      "Sun Oct 27 01:04:14 2019     batch 400   avg loss: 0.6020623308420181\n",
      "Sun Oct 27 01:04:24 2019     batch 600   avg loss: 0.6408211025595665\n",
      "Sun Oct 27 01:04:35 2019     batch 800   avg loss: 0.7245350863039494\n",
      "Sun Oct 27 01:04:43 2019   Validate:\n",
      "Sun Oct 27 01:04:59 2019     Avg loss for epoch: 0.9718629882210179   accuracy: 0.7242105263157895\n",
      "Sun Oct 27 01:04:59 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 01:04:59 2019   Saving model\n",
      "Sun Oct 27 01:04:59 2019   BEGIN EPOCH 14 of 20\n",
      "Sun Oct 27 01:04:59 2019   Train:\n",
      "Sun Oct 27 01:05:09 2019     batch 200   avg loss: 0.6110565501451493\n",
      "Sun Oct 27 01:05:19 2019     batch 400   avg loss: 0.6503753484785557\n",
      "Sun Oct 27 01:05:28 2019     batch 600   avg loss: 0.6485834062099457\n",
      "Sun Oct 27 01:05:38 2019     batch 800   avg loss: 0.6550838050246238\n",
      "Sun Oct 27 01:05:47 2019   Validate:\n",
      "Sun Oct 27 01:06:03 2019     Avg loss for epoch: 0.724584880377117   accuracy: 0.7747368421052632\n",
      "Sun Oct 27 01:06:03 2019     New accuracy peak, saving model\n",
      "Sun Oct 27 01:06:03 2019   Saving model\n",
      "Sun Oct 27 01:06:03 2019   BEGIN EPOCH 15 of 20\n",
      "Sun Oct 27 01:06:03 2019   Train:\n",
      "Sun Oct 27 01:06:12 2019     batch 200   avg loss: 0.4893761116266251\n",
      "Sun Oct 27 01:06:22 2019     batch 400   avg loss: 0.5289181782305241\n",
      "Sun Oct 27 01:06:33 2019     batch 600   avg loss: 0.5822609275579452\n",
      "Sun Oct 27 01:06:43 2019     batch 800   avg loss: 0.5843616065382957\n",
      "Sun Oct 27 01:06:51 2019   Validate:\n",
      "Sun Oct 27 01:07:07 2019     Avg loss for epoch: 0.8216160373938711   accuracy: 0.7368421052631579\n",
      "Sun Oct 27 01:07:07 2019   BEGIN EPOCH 16 of 20\n",
      "Sun Oct 27 01:07:07 2019   Train:\n",
      "Sun Oct 27 01:07:18 2019     batch 200   avg loss: 0.4618512910604477\n",
      "Sun Oct 27 01:07:27 2019     batch 400   avg loss: 0.5270386639237404\n",
      "Sun Oct 27 01:07:37 2019     batch 600   avg loss: 0.49891988053917885\n",
      "Sun Oct 27 01:07:46 2019     batch 800   avg loss: 0.5165941381454467\n",
      "Sun Oct 27 01:07:54 2019   Validate:\n",
      "Sun Oct 27 01:08:10 2019     Avg loss for epoch: 0.9085752684191654   accuracy: 0.716842105263158\n",
      "Sun Oct 27 01:08:10 2019   BEGIN EPOCH 17 of 20\n",
      "Sun Oct 27 01:08:10 2019   Train:\n",
      "Sun Oct 27 01:08:20 2019     batch 200   avg loss: 0.5012111563980579\n",
      "Sun Oct 27 01:08:30 2019     batch 400   avg loss: 0.47429988235235215\n",
      "Sun Oct 27 01:08:40 2019     batch 600   avg loss: 0.483572733849287\n",
      "Sun Oct 27 01:08:50 2019     batch 800   avg loss: 0.4860445936024189\n",
      "Sun Oct 27 01:08:58 2019   Validate:\n",
      "Sun Oct 27 01:09:14 2019     Avg loss for epoch: 0.8122715542190954   accuracy: 0.7631578947368421\n",
      "Sun Oct 27 01:09:14 2019   BEGIN EPOCH 18 of 20\n",
      "Sun Oct 27 01:09:14 2019   Train:\n",
      "Sun Oct 27 01:09:23 2019     batch 200   avg loss: 0.4546358358860016\n",
      "Sun Oct 27 01:09:33 2019     batch 400   avg loss: 0.4154538902640343\n",
      "Sun Oct 27 01:09:43 2019     batch 600   avg loss: 0.40736516505479814\n",
      "Sun Oct 27 01:09:53 2019     batch 800   avg loss: 0.33311455339193347\n",
      "Sun Oct 27 01:10:01 2019   Validate:\n",
      "Sun Oct 27 01:10:17 2019     Avg loss for epoch: 0.8869341875377454   accuracy: 0.7210526315789474\n",
      "Sun Oct 27 01:10:17 2019   BEGIN EPOCH 19 of 20\n",
      "Sun Oct 27 01:10:17 2019   Train:\n",
      "Sun Oct 27 01:10:27 2019     batch 200   avg loss: 0.4065872862935066\n",
      "Sun Oct 27 01:10:36 2019     batch 400   avg loss: 0.41322397768497465\n",
      "Sun Oct 27 01:10:48 2019     batch 600   avg loss: 0.4539711758494377\n",
      "Sun Oct 27 01:10:58 2019     batch 800   avg loss: 0.4286992000043392\n",
      "Sun Oct 27 01:11:04 2019   Validate:\n",
      "Sun Oct 27 01:11:20 2019     Avg loss for epoch: 0.8173533896396035   accuracy: 0.751578947368421\n",
      "Sun Oct 27 01:11:20 2019   BEGIN EPOCH 20 of 20\n",
      "Sun Oct 27 01:11:20 2019   Train:\n",
      "Sun Oct 27 01:11:30 2019     batch 200   avg loss: 0.34461541309952737\n",
      "Sun Oct 27 01:11:39 2019     batch 400   avg loss: 0.4584343771636486\n",
      "Sun Oct 27 01:11:51 2019     batch 600   avg loss: 0.41950547218322753\n",
      "Sun Oct 27 01:12:01 2019     batch 800   avg loss: 0.43510545790195465\n",
      "Sun Oct 27 01:12:08 2019   Validate:\n",
      "Sun Oct 27 01:12:24 2019     Avg loss for epoch: 0.8174178216331883   accuracy: 0.7526315789473684\n",
      "The best model is saved at seedling-e14-1572138363.pt with accuracy 0.7747368421052632\n"
     ]
    }
   ],
   "source": [
    "best_model_filename, accuracy  = train(model)\n",
    "print('The best model is saved at {} with accuracy {}'.format(best_model_filename, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ixGFv7Brqea"
   },
   "source": [
    "Load the model was trained and feed each of test instances and store the predictions into csv file in order to upload the csv on Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1572139814985,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "y7Smwgc6TIni",
    "outputId": "f8ae89bf-43bc-4d72-8431-abe1f8ed1ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeedlingModelV1(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=7744, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=12, bias=True)\n",
      ")\n",
      "tensor([[-4.5522,  4.5844,  2.0647, -0.8136, -3.1244, -1.3706, -4.2427, -0.7328,\n",
      "          5.1257,  4.9860, -0.4386, -1.2220]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "path = os.path.join(MODEL_DIR, '***')\n",
    "model_data = torch.load('/content/models/seedling-e14-1572138363.pt', map_location=torch.device('cpu'))\n",
    "trained_model = SeedlingModelV1()\n",
    "trained_model.load_state_dict(model_data)\n",
    "print(trained_model)\n",
    "\n",
    "# sanity check\n",
    "image, label = train_dataset[0]\n",
    "output = trained_model(torch.unsqueeze(image, 0))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1hOosQcnGxu"
   },
   "outputs": [],
   "source": [
    "!ls '/content/gdrive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1FSjJrzoVNp"
   },
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8DlZlAm3AEu"
   },
   "source": [
    "Load the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9qx1JfOYYl-"
   },
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "\n",
    "def get_test_transforms(target_size=100, normalize=False):\n",
    "    t = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "   \n",
    "    return t\n",
    "\n",
    "\n",
    "class SeedlingTestDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path_to_test_data=\"/content/gdrive/My Drive/curated/test/test\", transform=None):\n",
    "        self.transform = transform\n",
    "        self.data, self.datasize = self.build_dataset_from_path(path_to_test_data)\n",
    "        self.filenames = sorted(self.data.keys())\n",
    "\n",
    "    def build_dataset_from_path(self, test_data_path):\n",
    "        data = {}\n",
    "        for item in listdir(test_data_path):\n",
    "            file_path = join(test_data_path, item)\n",
    "            if isfile(file_path) and 'png' in file_path:\n",
    "                data[item] = file_path\n",
    "        return data, len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datasize\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.filenames[index]\n",
    "        full_path = self.data[key]\n",
    "\n",
    "        with open(full_path, 'rb') as f:\n",
    "            img = Image.open(BytesIO(f.read()))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzFgwJPh3Dcx"
   },
   "source": [
    "Instantiate the test dataset and wrap it in a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1658,
     "status": "ok",
     "timestamp": 1572141798819,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "6Y0FPPakZNLi",
    "outputId": "3c8d87f0-57cd-4ab7-8d46-8427a48cfe8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.0286,  2.4747,  2.2483, -0.7138, -5.1260,  1.7360, -4.1350, -1.7517,\n",
      "         -5.1252,  0.8126, 13.3818, -0.8629]], grad_fn=<AddmmBackward>)\n",
      "Small-flowered Cranesbill (score: 13.381834983825684)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SeedlingTestDataset(transform=get_transforms())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, num_workers=1, shuffle=False)\n",
    "\n",
    "# sanity check\n",
    "image, filename = test_dataset.__getitem__(0)\n",
    "output = trained_model(torch.unsqueeze(image, 0))\n",
    "print(output)\n",
    "\n",
    "# translate to a guess about class\n",
    "classes = full_dataset.classes\n",
    "score, pred = torch.max(output, 1)\n",
    "print('{} (score: {})'.format(classes[pred.item()], score.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBobNEzi3UMx"
   },
   "source": [
    "Making file submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54qTewZDa93n"
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w') as outfile:\n",
    "    outfile.write('file,species\\n') # required header row\n",
    "    \n",
    "    # Some models have layers that are only active during training,\n",
    "    # so always call model.eval() before inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, (data, filename) in enumerate(test_loader):\n",
    "            data.to(device)\n",
    "            output = trained_model(data)\n",
    "            score, pred = torch.max(output, 1)\n",
    "            outfile.write('{}, {}\\n'.format(filename[0], classes[pred.item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pzc25Ijl3YQ5"
   },
   "source": [
    "Showing the first instances of the file submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3735,
     "status": "ok",
     "timestamp": 1572142154542,
     "user": {
      "displayName": "Roberto Espinoza Valenzuela",
      "photoUrl": "",
      "userId": "07907011189372802316"
     },
     "user_tz": -630
    },
    "id": "3rAagC6acND1",
    "outputId": "e3af4123-24df-4534-9077-928175165f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file,species\n",
      "0021e90e4.png, Small-flowered Cranesbill\n",
      "003d61042.png, Fat Hen\n",
      "007b3da8b.png, Fat Hen\n",
      "0086a6340.png, Common Chickweed\n",
      "00c47e980.png, Sugar beet\n",
      "00d090cde.png, Scentless Mayweed\n",
      "00ef713a8.png, Common Chickweed\n",
      "01291174f.png, Fat Hen\n",
      "026716f9b.png, Loose Silky-bent\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8pUwiVy4vZ-"
   },
   "outputs": [],
   "source": [
    "!ls '/content/gdrive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0y12r-K3cUA"
   },
   "source": [
    "Downloading the csv file to the computer. Ready to uploaded to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ww2vNgTC56EH"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('submission.csv') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "First experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
